# PredicTalk
<img src="https://github.com/jphacks/NG_2305/assets/78719395/c3160c3a-5101-4182-8e85-7ab3148c5d12">

[こちら](https://youtu.be/Jx81Q2Q_JAw)からデモ動画をご覧いただけます。

## 目次
- [製品概要](#製品概要)
  - [開発背景](#開発背景)
  - [対象ユーザ](#対象ユーザ)
  - [特徴](#特徴)
  - 


## 製品概要
**「会話 × Tech」：初心者以上・ネイティブ未満の英語学習者を対象とした英会話を視覚的に補助するためのアプリ**

### 開発背景
日本人が外国人と英語で会話をする場合，英語をほとんど話せない初心者は翻訳アプリを利用します．  
言いたい日本語を入力すれば適切な英語が出力される上，発音が分からなくても機械音声に頼れるからです．  
ところが，英語がある程度話せるユーザの目線に立ってみると，翻訳アプリは英会話を補助するアプリとして十分な機能を果たしてるとは言えません．  
それは，彼らが持つ
- 基本的な単語や文法は知っているので**できるだけ自力で会話したい**
- けれど，実際に会話すると**単語や文法がスムーズに出てこない**
- でも，翻訳アプリをちらちら見ながら会話するのではなく**相手の表情や仕草を見てコミュニケーションしたい**
- それに，**勉強したのに結局翻訳アプリ頼りなのは歯がゆい**
というジレンマを解消できていないからです．

私たちはこのジレンマに着目し
- 基本的な単語や文法を知っており，できるだけ自力で相手を見ながら会話したいが，スムーズに話すスキルがないユーザ向けに
- **「相手を見ながら」** にして **「スムーズな英会話をするための補助が受けられる」**  
そんな製品開発を目指しました．

### 対象ユーザ
1. 日本人
1. 基本的な単語や文法の知識を持つ初心者以上・ネイティブ未満の英語学習者
1. 実際の英会話において「できるだけ自分の力で話したい」が「スムーズに単語や文法が出てこない」けど「翻訳アプリに頼りきりは嫌だ
」というニーズを持つ

### 特長
#### 特徴1:発話を補助しすぎません！
**「勉強内容を活用して自分の力で会話したい」** というニーズに応えるために，発話を補助しすぎないようにしています．  
具体的には，現在の発話から予想される続きの単語や文章の一部を表示することで，**「あくまでユーザに思い出させる」** ことを意識しています．  
**「"これまで勉強してきた単語や文法を活用して相手と同じ言語でやり取りできる喜び"を損なわない」** という点が，既存の翻訳アプリとの差別化になっています．

#### 特徴2：発話を視覚的に補助します！
相手の表情や仕草を見ることはコミュニケーションにおいて大切です．  
そのため，**発話を視覚的に補助するデバイスとしてARグラスを使用**することにしました．  
具体的には，現在の発話から予想される続きの単語や文章がARグラス上に透過表示されます．  
これにより，ユーザは相手を見ながらにして会話のサポートを受けられるようになります．

#### 特長3：速度と精度を両立しています！
本製品はユーザの発話を認識してそこから予想される単語や文章を生成します．  
したがって，ユーザのスムーズな会話を支援するために **「音声認識と予測機能の速度と精度を両立」** することが求められます． 
そこで，私たちはMoblieBertやGPT2などの複数の予測技術を実装して速度と精度のバランスを検証しました．
その結果，**音声認識の部分はAppleが提供するSpeach frameworkを，予測機能の部分は生成AIで有名なOpenAIが提供するGPT-3.5-Turbo APIを利用することで速度と精度のバランスを実現**しました．  

### 製品説明（具体的な製品の説明）
会話の続きを予測するiOSアプリ 

<img width="590" alt="スクリーンショット 2023-10-30 4 05 47" src="https://github.com/jphacks/NG_2305/assets/78719395/8db669c8-8221-42f5-b3f7-cdd60c2bebc4">

- アプリを起動して画面をタップすると音声認識が開始されます。
- 自身の話した内容は白色の文字で表示されます。
  
<img width="596" alt="スクリーンショット 2023-10-30 4 04 02" src="https://github.com/jphacks/NG_2305/assets/78719395/18f8ca2f-8d62-4b54-a269-0b932a0f29d6">

- 会話の続きを予測した内容は薄い白色の文字で表示されます。

  ![IMG_7924](https://github.com/jphacks/NG_2305/assets/109562639/35ddd113-a542-46d0-9a0f-c078bbbcd8f7)

- ユーザはARグラスに表示された単語を見ることで，思い出せなかった単語や文法を思い出すことで発話をスムーズに行うことができます。これはXREAL社のairというARグラスを着用している様子です！
  
<img width="1285" alt="スクリーンショット 2023-10-30 4 34 26" src="https://github.com/jphacks/NG_2305/assets/109562639/c99497f8-dbb3-4ae8-a9b1-bd5876359a21">

- ARグラスとIphoneを接続します。ARグラスの着用時には背景が透過され，文章だけが表示されます!

### 他の方法との差別化
音声認識と単語予測、さらにARグラスを組み合わせることで、円滑なコミュニケーションを可能としています！
- 翻訳アプリを介した会話
  - お互いがスマホに向かい合っていると、相手の表情や仕草を見ることができない
  - 毎回スマホを見ながら会話するのは疲れてしまう
  - なんとも言えない違和感がある
- 自分の言いたいことを翻訳アプリに通して出力された文章を発音する
  - 自分の力で会話しているとは言い難く、相手とコミュニケーションしている実感が得られない。
  - スムーズな会話ができない
  - 基本的な語学知識はあるのに翻訳アプリに頼りきりなのはもどかしい

### 解決出来ること
初心者以上・ネイティブ未満の外国語学習者が持っている「**学んだ外国語を利用してコミュニケーションしたい**」という思いを尊重し、「実際の会話で単語や文法がスムーズに出てこない」でも「せっかく勉強したのに翻訳アプリにすべて頼って会話するのはもったいない」というジレンマを解決します！
  
また、ただ単語予測をスマホ画面に表示して補助するのではなく、市販のARグラスに投影する形で表示することでコミュニケーションを円滑にするという新しい視点を提供します！


本アプリでは、**グラスをかけて画面をタップするだけ**で、サービスを提供します！
### 今後の展望
今後の展望としては2つの改善点が挙げられます。  
  
1つ目は、**対話内容に基づく予測単語の精度向上**です。  
スマホやPCの予測変換機能はユーザの入力履歴に基づいて予測の精度が向上します。
本製品もこの技術の考え方を利用して、**現在の対話内容や過去の対話履歴に基づいた予測**をすることで精度を向上することを考えています。
  
2つ目は、**UIやその他の機能を充実させること**です。
UIに時間を割いて開発を行うことができなかったため、現在のアプリではUIが質素なものになっています。
**より凝ったデザインを実装**することで、ユーザが使っていて楽しいアプリにすることを目指したいと思っています。
また、2日間の開発ではARグラスの使用を想定した最低限の機能しか実装することができませんでした。
**アプリ単体でのモード**や、**多言語対応**などといった機能も実装していきたいと考えています。

### 注力したこと（こだわり等）
- **次文予測機能の実装**
    - 初めは、速度を担保するためにMobileBertやGPT2といった言語モデルをデバイス上で動かすことを検討しましたが、MobileBertは精度がGPT2は速度が十分なものではありませんでした。
    - 最終的には、OpenAIのAPIを使ってgpt-3.5-turboを使用することで、精度と速度の双方を担保した実装をすることができました。
- **新たな情報源の導入**
    - 市販のARデバイスで使用できるアプリにしたことで、本製品の目的である**コミュニケーションのスムーズさ**を妨害しないで発話の補助を達成しました。
- **視覚的に邪魔になる情報の削除**
    - 音声認識をスタートするためのタップの判定エリアを画面全体にすることで、話しながらでの操作が可能となっています。
    - 画面には自分が話した内容とその文章の続きしか表示されないので、余計な情報がなく、会話中にも使用しやすいUXを意識しました。

## 開発技術
<img width="971" alt="スクリーンショット 2023-10-30 4 51 16" src="https://github.com/jphacks/NG_2305/assets/109562639/807cb967-b25d-4235-88d1-c94052978d7e">

### 活用した技術
#### API・データ
- OpenAI API(gpt-3.5-turbo)
    - ユーザの話した文章の続きを生成させるために使用
    - 自身の環境で本アプリを動作させる場合には、OpenAIAPIKey.swiftに自身のOpenAI API Keyを入力してください。
```Swift:OpenAIAPIKey.swift
//  OpenAIAPIKey.swift

let API_KEY = "<Input Your API Key Here>"
```

#### フレームワーク・ライブラリ・モジュール
- Speech
    - Swiftの純正フレームワークで、音声処理に関するフレームワークです。
    - ユーザの話した内容を文字に起こすために使用しました。
- CoreML・CoreMLTools
    - CoreMLはSwiftの純正フレームワークで、機械学習モデルをSwift上で使用するためのフレームワークです。
    - CoreMLToolsはPythonライブラリで、PytorchやTensorflowで作られたニューラルネットワークモデルをCoreML形式へ変換するためのツールです。
    - これらのツールはMobileBertとGPT2をCoreML形式へ変換し、iOS端末上で動作させるようにするために使用しました。
- Moya
    - OpenAIのAPIをSwiftを使って叩くために使用しました。

#### デバイス
- XREAL air
    - アプリの出力をユーザの視界内に表示するために使用しました。

### 独自技術
#### ハッカソンで開発した独自機能・技術
##### 音声認識結果を保存する機能
commit_id: [5674e3c7c945d6a1c6ec8a8ade41fcb9ef19ea1d](https://github.com/jphacks/NG_2305/commit/5674e3c7c945d6a1c6ec8a8ade41fcb9ef19ea1d)

音声認識自体はSpeechフレームワークを用いて実装したのですが、音声認識結果を保存する機能を独自に実装しました。

SFSpeechRecognizerを用いて音声をテキストに起こす際に、Speechフレームワークの仕様により音声認識の途中結果をいくつも出力してしまったり、時間経過により認識された文章が削除されてしまい、認識結果が重複して画面に表示されたり話すのを少しでも止めるとこれまでの認識結果が失われるといった問題がありました。

これらを解決するために、音声認識の途中結果をバッファリングしておき、認識が完了したこと検知したら最終結果を画面に出力したり、時間経過で認識された文章が削除されないように、認識結果を保存しておく機能を実装しました。

#### 製品に取り入れた研究内容（データ・ソフトウェアなど）（※アカデミック部門の場合のみ提出必須）
アカデミック部門ではないため無し






